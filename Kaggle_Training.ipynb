{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf74615",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment & Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665125ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas numpy scikit-learn matplotlib seaborn torch transformers datasets tqdm joblib\n",
    "\n",
    "# Verify GPU availability\n",
    "import torch\n",
    "print(\"=\" * 60)\n",
    "print(\"GPU CHECK\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(\"\\n‚úì GPU is ready for training!\")\n",
    "else:\n",
    "    print(\"\\n‚ö† WARNING: GPU not detected!\")\n",
    "    print(\"Enable GPU: Settings ‚Üí Accelerator ‚Üí GPU T4 x2\")\n",
    "    print(\"Training will be MUCH slower on CPU.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b5e72",
   "metadata": {},
   "source": [
    "## Step 2: Access Project Files\n",
    "\n",
    "**Option 1: Add Dataset (Recommended)**\n",
    "- Click \"+ Add Data\" in the right panel\n",
    "- Upload your dataset or search for existing ones\n",
    "- Your files will be in `/kaggle/input/your-dataset-name/`\n",
    "\n",
    "**Option 2: Direct File Access**\n",
    "- If you've already added your dataset, this cell will find it automatically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ACCESSING PROJECT FILES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Look for Kaggle input directory\n",
    "kaggle_input = '/kaggle/input'\n",
    "source_dir = None\n",
    "\n",
    "if os.path.exists(kaggle_input):\n",
    "    # Find the dataset folder\n",
    "    datasets = [d for d in os.listdir(kaggle_input) if os.path.isdir(os.path.join(kaggle_input, d))]\n",
    "    if datasets:\n",
    "        source_dir = os.path.join(kaggle_input, datasets[0])\n",
    "        print(f\"‚úì Found dataset: {datasets[0]}\")\n",
    "        print(f\"  Location: {source_dir}\")\n",
    "    else:\n",
    "        print(\"‚úó No datasets found in /kaggle/input/\")\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üí° HOW TO ADD YOUR DATA:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nüìÅ STEP-BY-STEP INSTRUCTIONS:\")\n",
    "        print(\"\\n1. Look for the '+ Add Data' button on the RIGHT side of this page\")\n",
    "        print(\"2. Click it and select 'Upload' tab\")\n",
    "        print(\"3. Upload your ZIP file containing:\")\n",
    "        print(\"   - train_all_models.py\")\n",
    "        print(\"   - data/processed/medical_dataset.csv (or just medical_dataset.csv)\")\n",
    "        print(\"   - utils/ folder\")\n",
    "        print(\"4. Click 'Add Dataset' button\")\n",
    "        print(\"5. Wait for upload to complete (may take a few minutes)\")\n",
    "        print(\"6. Re-run this cell\")\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"\\n‚è∏Ô∏è  PAUSED: Waiting for you to add data...\")\n",
    "        print(\"This is NOT an error - just follow the steps above!\")\n",
    "        print(\"=\" * 60)\n",
    "        raise SystemExit(\"Please add your dataset and re-run this cell.\")\n",
    "else:\n",
    "    print(\"‚úó Not running in Kaggle environment\")\n",
    "    print(\"\\nüí° This notebook is designed for Kaggle.\")\n",
    "    print(\"If you're running locally, you'll need to modify the file paths.\")\n",
    "    raise EnvironmentError(\"This notebook is designed for Kaggle. /kaggle/input/ not found.\")\n",
    "\n",
    "# Check if it's a ZIP file that needs extraction\n",
    "zip_files = [f for f in os.listdir(source_dir) if f.endswith('.zip')]\n",
    "if zip_files and not os.path.exists(os.path.join(source_dir, 'train_all_models.py')):\n",
    "    print(f\"\\nüì¶ Found ZIP file: {zip_files[0]}\")\n",
    "    print(\"Extracting to working directory...\")\n",
    "    with zipfile.ZipFile(os.path.join(source_dir, zip_files[0]), 'r') as zip_ref:\n",
    "        zip_ref.extractall('extracted')\n",
    "    source_dir = 'extracted'\n",
    "    print(\"‚úì Extraction complete\")\n",
    "\n",
    "# Find project root if files are in subdirectory\n",
    "if not os.path.exists(os.path.join(source_dir, 'train_all_models.py')):\n",
    "    print(\"\\nSearching for project files in subdirectories...\")\n",
    "    found = False\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        if 'train_all_models.py' in files:\n",
    "            source_dir = root\n",
    "            print(f\"‚úì Found project files in: {source_dir}\")\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        print(\"‚úó train_all_models.py not found\")\n",
    "        print(\"\\nüìÇ Available files in dataset:\")\n",
    "        for root, dirs, files in os.walk(source_dir):\n",
    "            for file in files[:10]:  # Show first 10 files\n",
    "                print(f\"  - {os.path.join(root, file)}\")\n",
    "        raise FileNotFoundError(\"train_all_models.py not found in dataset\")\n",
    "\n",
    "print(f\"\\n‚úì Source directory: {source_dir}\")\n",
    "\n",
    "if not os.path.exists(source_dir):\n",
    "    raise FileNotFoundError(f\"Source directory not found: {source_dir}\")\n",
    "\n",
    "# Copy project files to working directory\n",
    "print(\"\\nCopying files to working directory...\")\n",
    "\n",
    "# Copy train_all_models.py\n",
    "if os.path.exists(os.path.join(source_dir, 'train_all_models.py')):\n",
    "    shutil.copy2(os.path.join(source_dir, 'train_all_models.py'), 'train_all_models.py')\n",
    "    print(\"‚úì Copied train_all_models.py\")\n",
    "else:\n",
    "    print(\"‚úó train_all_models.py not found in source\")\n",
    "\n",
    "# Copy utils folder\n",
    "if os.path.exists(os.path.join(source_dir, 'utils')):\n",
    "    if os.path.exists('utils'):\n",
    "        shutil.rmtree('utils')\n",
    "    shutil.copytree(os.path.join(source_dir, 'utils'), 'utils')\n",
    "    utils_files = len(os.listdir('utils'))\n",
    "    print(f\"‚úì Copied utils/ folder ({utils_files} files)\")\n",
    "else:\n",
    "    print(\"‚úó utils/ folder not found in source\")\n",
    "\n",
    "# Handle medical_dataset.csv - check multiple possible locations\n",
    "csv_found = False\n",
    "possible_csv_paths = [\n",
    "    os.path.join(source_dir, 'data', 'processed', 'medical_dataset.csv'),\n",
    "    os.path.join(source_dir, 'medical_dataset.csv')\n",
    "]\n",
    "\n",
    "for csv_path in possible_csv_paths:\n",
    "    if os.path.exists(csv_path):\n",
    "        os.makedirs('data/processed', exist_ok=True)\n",
    "        shutil.copy2(csv_path, 'data/processed/medical_dataset.csv')\n",
    "        print(f\"‚úì Copied medical_dataset.csv\")\n",
    "        csv_found = True\n",
    "        break\n",
    "\n",
    "if not csv_found:\n",
    "    # Search for CSV file anywhere in source directory\n",
    "    print(\"Searching for medical_dataset.csv...\")\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        if 'medical_dataset.csv' in files:\n",
    "            csv_path = os.path.join(root, 'medical_dataset.csv')\n",
    "            os.makedirs('data/processed', exist_ok=True)\n",
    "            shutil.copy2(csv_path, 'data/processed/medical_dataset.csv')\n",
    "            print(f\"‚úì Found and copied medical_dataset.csv\")\n",
    "            csv_found = True\n",
    "            break\n",
    "    \n",
    "    if not csv_found:\n",
    "        print(\"‚úó medical_dataset.csv not found in source\")\n",
    "\n",
    "# Verify copied files\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FILE VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists('train_all_models.py'):\n",
    "    size = os.path.getsize('train_all_models.py')\n",
    "    print(f\"‚úì train_all_models.py ({size:,} bytes)\")\n",
    "else:\n",
    "    print(\"‚úó train_all_models.py - MISSING\")\n",
    "\n",
    "if os.path.exists('data/processed/medical_dataset.csv'):\n",
    "    size = os.path.getsize('data/processed/medical_dataset.csv')\n",
    "    print(f\"‚úì medical_dataset.csv ({size:,} bytes)\")\n",
    "else:\n",
    "    print(\"‚úó medical_dataset.csv - MISSING\")\n",
    "\n",
    "if os.path.exists('utils'):\n",
    "    utils_files = len(os.listdir('utils'))\n",
    "    print(f\"‚úì utils/ folder ({utils_files} files)\")\n",
    "else:\n",
    "    print(\"‚úó utils/ folder - MISSING\")\n",
    "\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5622e",
   "metadata": {},
   "source": [
    "## Step 3: Verify Dataset\n",
    "\n",
    "Check that your dataset is ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f70f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dataset_path = 'data/processed/medical_dataset.csv'\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"‚úó ERROR: Dataset not found at {dataset_path}\")\n",
    "    raise FileNotFoundError(f\"Dataset not found: {dataset_path}\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(f\"‚úì Dataset loaded successfully\")\n",
    "    print(f\"\\nTotal samples: {len(df):,}\")\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(\"\\n‚úó ERROR: Dataset is empty\")\n",
    "        raise ValueError(\"Dataset is empty\")\n",
    "    \n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    \n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    label_counts = df['label'].value_counts()\n",
    "    for label, count in label_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  {label:12s}: {count:6,} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n‚úì Dataset is ready for training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó ERROR: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f296f74",
   "metadata": {},
   "source": [
    "## Step 4: Clean Previous Results\n",
    "\n",
    "Delete any old training results to start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faeabc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLEANING PREVIOUS TRAINING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dirs_to_clean = ['models', 'results', 'logs']\n",
    "\n",
    "for dir_path in dirs_to_clean:\n",
    "    if os.path.exists(dir_path):\n",
    "        try:\n",
    "            shutil.rmtree(dir_path)\n",
    "            print(f\"‚úì Deleted: {dir_path}/\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error deleting {dir_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"  {dir_path}/ does not exist\")\n",
    "\n",
    "# Recreate fresh directory structure\n",
    "os.makedirs('models/ml', exist_ok=True)\n",
    "os.makedirs('models/dl', exist_ok=True)\n",
    "os.makedirs('models/transformer', exist_ok=True)\n",
    "os.makedirs('results/ml', exist_ok=True)\n",
    "os.makedirs('results/dl', exist_ok=True)\n",
    "os.makedirs('results/transformer', exist_ok=True)\n",
    "\n",
    "print(\"\\n‚úì All previous results deleted\")\n",
    "print(\"‚úì Fresh directories created\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75653548",
   "metadata": {},
   "source": [
    "## Step 5: Train All Models\n",
    "\n",
    "This will train all 5 models:\n",
    "- **ML Models:** Logistic Regression, Random Forest\n",
    "- **DL Models:** CNN, LSTM\n",
    "- **Transformer:** BioBERT\n",
    "\n",
    "**Training improvements applied:**\n",
    "- Label smoothing (0.1) to reduce overconfidence\n",
    "- Stronger regularization (L2, dropout, weight decay)\n",
    "- Lower learning rates for better convergence\n",
    "- Early stopping to prevent overfitting\n",
    "\n",
    "‚è± **Expected time:** 1-3 hours depending on dataset size and GPU\n",
    "\n",
    "You can monitor progress below. The training will show:\n",
    "- Current model being trained\n",
    "- Epoch progress\n",
    "- Training and validation metrics\n",
    "- Final test results for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Disable W&B (Weights & Biases) tracking to avoid interactive prompts\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Training 5 models:\")\n",
    "print(\"  1. Logistic Regression (ML)\")\n",
    "print(\"  2. Random Forest (ML)\")\n",
    "print(\"  3. CNN (Deep Learning)\")\n",
    "print(\"  4. LSTM (Deep Learning)\")\n",
    "print(\"  5. BioBERT (Transformer)\")\n",
    "print(\"\\nThis will take 1-3 hours. Progress shown below...\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "!python train_all_models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdea832",
   "metadata": {},
   "source": [
    "## Step 6: Verify Training Results\n",
    "\n",
    "Check that all models and results were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING RESULTS VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for trained models\n",
    "expected_models = {\n",
    "    'ML Models': [\n",
    "        'models/ml/logistic_regression.pkl',\n",
    "        'models/ml/random_forest.pkl',\n",
    "        'models/ml/tfidf_vectorizer.pkl',\n",
    "        'models/ml/label_encoder.pkl'\n",
    "    ],\n",
    "    'DL Models': [\n",
    "        'models/dl/cnn_best.pt',\n",
    "        'models/dl/lstm_best.pt',\n",
    "        'models/dl/vocab.json'\n",
    "    ],\n",
    "    'Transformer': [\n",
    "        'models/transformer/biobert_final/config.json',\n",
    "        'models/transformer/biobert_final/tokenizer_config.json'\n",
    "    ]\n",
    "}\n",
    "\n",
    "expected_results = {\n",
    "    'ML Results': [\n",
    "        'results/ml/logistic_regression_metrics.json',\n",
    "        'results/ml/random_forest_metrics.json'\n",
    "    ],\n",
    "    'DL Results': [\n",
    "        'results/dl/cnn_metrics.json',\n",
    "        'results/dl/lstm_metrics.json'\n",
    "    ],\n",
    "    'Transformer Results': [\n",
    "        'results/transformer/biobert_metrics.json'\n",
    "    ]\n",
    "}\n",
    "\n",
    "all_good = True\n",
    "\n",
    "print(\"\\nüìÅ MODELS:\")\n",
    "for category, files in expected_models.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for file_path in files:\n",
    "        if os.path.exists(file_path):\n",
    "            size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "            print(f\"  ‚úì {os.path.basename(file_path)} ({size:.2f} MB)\")\n",
    "        else:\n",
    "            print(f\"  ‚úó {os.path.basename(file_path)} - MISSING\")\n",
    "            all_good = False\n",
    "\n",
    "# Check transformer model weights\n",
    "transformer_dir = 'models/transformer/biobert_final'\n",
    "if os.path.exists(transformer_dir):\n",
    "    transformer_files = os.listdir(transformer_dir)\n",
    "    model_file = next((f for f in transformer_files if f.endswith('.safetensors') or f.endswith('.bin')), None)\n",
    "    if model_file:\n",
    "        size = os.path.getsize(os.path.join(transformer_dir, model_file)) / (1024 * 1024)\n",
    "        print(f\"  ‚úì {model_file} ({size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ‚úó BioBERT weights - MISSING\")\n",
    "        all_good = False\n",
    "\n",
    "print(\"\\nüìä RESULTS:\")\n",
    "for category, files in expected_results.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for file_path in files:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"  ‚úì {os.path.basename(file_path)}\")\n",
    "        else:\n",
    "            print(f\"  ‚úó {os.path.basename(file_path)} - MISSING\")\n",
    "            all_good = False\n",
    "\n",
    "# Show summary metrics if available\n",
    "summary_file = 'results/comprehensive_training_summary.json'\n",
    "if os.path.exists(summary_file):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    with open(summary_file, 'r') as f:\n",
    "        summary = json.load(f)\n",
    "    \n",
    "    print(f\"\\nTotal models trained: {summary['project_info']['total_models']}\")\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    for model_key, model_data in summary['models'].items():\n",
    "        metrics = model_data['test_set_metrics']\n",
    "        print(f\"\\n{model_data['model_name'].upper()}:\")\n",
    "        print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  F1-Macro:  {metrics['f1_macro']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision_macro']:.4f}\")\n",
    "        print(f\"  Recall:    {metrics['recall_macro']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if all_good:\n",
    "    print(\"‚úì ALL MODELS TRAINED SUCCESSFULLY!\")\n",
    "else:\n",
    "    print(\"‚ö† WARNING: Some files are missing\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1b3ea",
   "metadata": {},
   "source": [
    "## Step 7: Download Results\n",
    "\n",
    "Package all trained models and results into a ZIP file and download directly. Progress updates keep the session alive during packaging to prevent timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcfcffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import time\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPARING DOWNLOAD PACKAGE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚è≥ Packaging files... (this keeps session alive)\")\n",
    "\n",
    "# Create temporary directory for packaging\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_dir = f'Medical_Misinformation_Training_{timestamp}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "files_copied = 0\n",
    "last_update = time.time()\n",
    "\n",
    "def print_progress(message):\n",
    "    \"\"\"Print progress to keep session alive\"\"\"\n",
    "    print(f\"  {message}\")\n",
    "\n",
    "# Copy models folder with progress\n",
    "if os.path.exists('models'):\n",
    "    print(\"\\nüìÅ Copying models...\")\n",
    "    for root, dirs, files in os.walk('models'):\n",
    "        for file in files:\n",
    "            src = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(src, 'models')\n",
    "            dst = os.path.join(output_dir, 'models', rel_path)\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "            shutil.copy2(src, dst)\n",
    "            files_copied += 1\n",
    "            # Print every 5 files to show activity\n",
    "            if files_copied % 5 == 0:\n",
    "                print_progress(f\"Copied {files_copied} files...\")\n",
    "    print(f\"  ‚úì Copied models/ ({files_copied} files)\")\n",
    "\n",
    "# Copy results folder with progress\n",
    "if os.path.exists('results'):\n",
    "    print(\"\\nüìä Copying results...\")\n",
    "    result_start = files_copied\n",
    "    for root, dirs, files in os.walk('results'):\n",
    "        for file in files:\n",
    "            src = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(src, 'results')\n",
    "            dst = os.path.join(output_dir, 'results', rel_path)\n",
    "            os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "            shutil.copy2(src, dst)\n",
    "            files_copied += 1\n",
    "    result_count = files_copied - result_start\n",
    "    print(f\"  ‚úì Copied results/ ({result_count} files)\")\n",
    "\n",
    "# Copy dataset\n",
    "if os.path.exists('data/processed/medical_dataset.csv'):\n",
    "    print(\"\\nüìÑ Copying dataset...\")\n",
    "    os.makedirs(os.path.join(output_dir, 'data'), exist_ok=True)\n",
    "    shutil.copy2('data/processed/medical_dataset.csv', \n",
    "                 os.path.join(output_dir, 'data', 'medical_dataset.csv'))\n",
    "    files_copied += 1\n",
    "    print(f\"  ‚úì Copied dataset\")\n",
    "\n",
    "# Create README\n",
    "readme_content = f\"\"\"# Medical Misinformation Detection - Training Results\n",
    "Training completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Contents:\n",
    "\n",
    "### Models ({files_copied} files total)\n",
    "- models/ml/ - Logistic Regression & Random Forest\n",
    "- models/dl/ - CNN & LSTM (PyTorch)\n",
    "- models/transformer/ - BioBERT\n",
    "\n",
    "### Results\n",
    "- results/ml/ - ML model metrics and confusion matrices\n",
    "- results/dl/ - DL model metrics, confusion matrices, and training curves\n",
    "- results/transformer/ - BioBERT metrics and confusion matrix\n",
    "- results/comprehensive_training_summary.json - Complete summary\n",
    "\n",
    "### Data\n",
    "- data/medical_dataset.csv - Dataset used for training\n",
    "\n",
    "## How to Use:\n",
    "1. Extract this ZIP file on your local machine\n",
    "2. Review metrics in results/ folder\n",
    "3. Load models in your application:\n",
    "   - ML models: joblib.load()\n",
    "   - DL models: torch.load()\n",
    "   - Transformer: AutoModel.from_pretrained()\n",
    "\"\"\"\n",
    "\n",
    "readme_path = os.path.join(output_dir, 'README.txt')\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "# Create ZIP file with progress updates\n",
    "print(\"\\nüì¶ Creating ZIP file...\")\n",
    "print(\"  (Printing progress to keep session alive)\")\n",
    "zip_filename = f'{output_dir}.zip'\n",
    "\n",
    "files_to_zip = []\n",
    "for root, dirs, files in os.walk(output_dir):\n",
    "    for file in files:\n",
    "        files_to_zip.append(os.path.join(root, file))\n",
    "\n",
    "total_files = len(files_to_zip)\n",
    "print(f\"  Compressing {total_files} files...\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for idx, file_path in enumerate(files_to_zip, 1):\n",
    "        arcname = os.path.relpath(file_path, os.path.dirname(output_dir))\n",
    "        zipf.write(file_path, arcname)\n",
    "        # Print progress every 10 files to keep session alive\n",
    "        if idx % 10 == 0 or idx == total_files:\n",
    "            print(f\"  Progress: {idx}/{total_files} files compressed...\")\n",
    "\n",
    "zip_size = os.path.getsize(zip_filename) / (1024 * 1024)  # MB\n",
    "print(f\"\\n‚úì ZIP file created: {zip_filename}\")\n",
    "print(f\"  Size: {zip_size:.2f} MB\")\n",
    "print(f\"  Total files: {files_copied}\")\n",
    "\n",
    "# Clean up temporary directory\n",
    "shutil.rmtree(output_dir)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì PACKAGE READY FOR DOWNLOAD!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüì• DOWNLOADING...\")\n",
    "print(\"  Click the link below to download:\")\n",
    "print(\"  (If download doesn't start, right-click ‚Üí Save Link As)\")\n",
    "print()\n",
    "\n",
    "# Display download link\n",
    "display(FileLink(zip_filename))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì Download link displayed above\")\n",
    "print(\"  The file will also appear in the Output tab\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c0336",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "Your models have been trained and packaged for download.\n",
    "\n",
    "**ZIP file:** `Medical_Misinformation_Training_[timestamp].zip`\n",
    "\n",
    "### Contents:\n",
    "\n",
    "**Models:**\n",
    "- `models/ml/` - Logistic Regression & Random Forest\n",
    "- `models/dl/` - CNN & LSTM\n",
    "- `models/transformer/` - BioBERT\n",
    "\n",
    "**Results:**\n",
    "- `results/ml/` - ML model metrics and confusion matrices\n",
    "- `results/dl/` - DL model metrics, confusion matrices, and training curves\n",
    "- `results/transformer/` - BioBERT metrics and confusion matrix\n",
    "- `results/comprehensive_training_summary.json` - Complete summary\n",
    "\n",
    "**Data:**\n",
    "- `data/medical_dataset.csv` - Dataset used for training\n",
    "\n",
    "### Download Instructions:\n",
    "\n",
    "1. **Click the download link in the output above** (Step 7 cell output)\n",
    "2. If link doesn't work, **right-click ‚Üí Save Link As...**\n",
    "3. **Alternative:** Check the **Output** tab (top-right) for the ZIP file\n",
    "\n",
    "### Next Steps:\n",
    "1. Extract the ZIP file on your local machine\n",
    "2. Review metrics in the `results/` folder\n",
    "3. Use the trained models for predictions in your application\n",
    "\n",
    "---\n",
    "\n",
    "**Need to retrain?** Re-run from Step 5 (training cell)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
